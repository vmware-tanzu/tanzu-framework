#@data/values

#@overlay/match-child-defaults missing_ok=True
---

#! ---------------------------------------------------------------------
#! List of variables used in cluster configuration and their default
#! values, where available. It is recommended that overrides to any of
#! these values be done is the cluster configuration file instead.
#! ---------------------------------------------------------------------



#! ---------------------------------------------------------------------
#! Cluster creation basic configuration
#! ---------------------------------------------------------------------
#! Cluster name to use when deploying cluster.
#! CLUSTER_NAME is required while creating workload cluster
CLUSTER_NAME:
#! Plan to use when creating cluster. Supported values 'dev', 'prod'
CLUSTER_PLAN:
#! Namespace to use to deploy workload cluster
#! if not provided `default` namespace will be used for workload cluster
NAMESPACE:
#! Supported infrastructure provider values are 'vsphere', 'aws', 'azure'
#! INFRASTRUCTURE_PROVIDER is required while creating management cluster
INFRASTRUCTURE_PROVIDER:
#! API Server Port number to be configured for the cluster.
#! if not provided default 6443 would be used.
#! if AVI load balancer is used for vSphere provider, VSPHERE_CONTROL_PLANE_ENDPOINT_PORT would
#! be used instead
CLUSTER_API_SERVER_PORT:

CLUSTER_CLASS:

#! Instance size configuration
#! Instance size options available for vSphere are as follows:
#!   vSphere: [extra-large,large,medium,small]

#! Specify size for all nodes including control plane and worker nodes.
#! It can be overridden by CONTROLPLANE_SIZE and WORKER_SIZE config variables
SIZE:
#! Specify size for the control plane node
CONTROLPLANE_SIZE:
#! Specify size of the worker node
WORKER_SIZE:

#! Management cluster creation specific configuration, does not apply to workload clusters

#! VMware's Customer Experience Improvement Program ("CEIP") provides VMware with information that enables
#! VMware to improve its products and services and fix problems. By choosing to participate in CEIP, you agree that
#! VMware may collect technical information about your use of VMware products and services on a regular basis. This
#! information does not personally identify you.
#! Specify if this management cluster should participate in VMware CEIP. (default true)
ENABLE_CEIP_PARTICIPATION:
#! Deploy TKG Management cluster on vSphere 7.0 without prompt
DEPLOY_TKG_ON_VSPHERE7:
#! Enable TKGS on vSphere 7.0 without prompt
ENABLE_TKGS_ON_VSPHERE7:

#! BUILD_EDITION is the Tanzu Edition, the plugin should be built for.
#! Valid values for BUILD_EDITION are 'tce' and 'tkg'.
BUILD_EDITION:

#! ---------------------------------------------------------------------
#! Settings for vSphere infrastructure provider
#! ---------------------------------------------------------------------

#! CPU,disk,memory values to be used for creating cluster VMs
#! this can be overridden by defining the VSPHERE_WORKER_DISK_GIB for the workers
#! and VSPHERE_CONTROL_PLANE_DISK_GIB for the control plane
VSPHERE_NUM_CPUS: 2
VSPHERE_DISK_GIB: 40
VSPHERE_MEM_MIB: 4096

#! Specific overrides of the above settings that only apply control plane
#! or worker nodes
VSPHERE_CONTROL_PLANE_NUM_CPUS: 2
VSPHERE_CONTROL_PLANE_DISK_GIB: 40
VSPHERE_CONTROL_PLANE_MEM_MIB: 8192

VSPHERE_WORKER_NUM_CPUS: 2
VSPHERE_WORKER_DISK_GIB: 40
VSPHERE_WORKER_MEM_MIB: 4096

VSPHERE_CLONE_MODE: "fullClone"

#! The network portgroup to assign each VM node
VSPHERE_NETWORK: VM Network

#! The name of the VM template to be used to create a specific version of a Tanzu
#! Kubernetes Cluster. Since the appropriate template is automatically
#! discovered from the vCenter inventory, the use of this setting is no longer
#! necessary except to disambiguate among 2 or more applicable VM templates.
VSPHERE_TEMPLATE:
VSPHERE_TEMPLATE_MOID:

#! WINDOWS PARAMETERS
#! The name of the VM Windows template to be used to create a specific version of
#! a Tanzu Kubernetes Cluster including WINDOWS node.
VSPHERE_WINDOWS_TEMPLATE: ""
IS_WINDOWS_WORKLOAD_CLUSTER: false

VIP_NETWORK_INTERFACE: "eth0"

#! The contents of the public key to be injected into the VM nodes created
VSPHERE_SSH_AUTHORIZED_KEY:
VSPHERE_USERNAME:
VSPHERE_PASSWORD:

#! The content of zone and region information is used to deploy CPI and CSI in a
#! vSphere environment that includes multiple data centers or host clusters.
VSPHERE_REGION:
VSPHERE_ZONE:

VSPHERE_AZ_0:
VSPHERE_AZ_1:
VSPHERE_AZ_2:

#! FQDN or IP address to vCenter
VSPHERE_SERVER:

#! Full inventory path or names are supported for the following
#! the former is required if there are multiple entities of the same type and
#! name in the VC inventory.
VSPHERE_DATACENTER:
#! Name of an existing resource pool in which to place this Tanzu Kubernetes cluster
VSPHERE_RESOURCE_POOL:
#! Name of the vSphere datastore to deploy the Tanzu Kubernetes cluster
#! as it appears in the vSphere inventory
VSPHERE_DATASTORE:
#! name of an existing VM folder in which to place Tanzu Kubernetes Grid VMs
VSPHERE_FOLDER:
VSPHERE_STORAGE_POLICY_ID: ""

#! Device and vendor ids for GPU PCI passthrough support
#! VSPHERE_WORKER_PCI_DEVICES, if set will configure PCI passthrough on all worker machines
#! VSPHERE_WORKER_PCI_DEVICES must be specified in the format 0x<vendor_id>:0x<device_id>
#! e.g. VSPHERE_WORKER_PCI_DEVICES: "0x10DE:0x1EB8"
VSPHERE_WORKER_PCI_DEVICES:
#! VSPHERE_CONTROL_PLANE_PCI_DEVICES, if set will configure PCI passthrough on all control plane machines
#! VSPHERE_CONTROL_PLANE_PCI_DEVICES must be specified in the format <vendor_id>:<device_id>
#! e.g. VSPHERE_CONTROL_PLANE_PCI_DEVICES: "0x10DE:0x1EB8"
VSPHERE_CONTROL_PLANE_PCI_DEVICES:
#! VSPHERE_IGNORE_PCI_DEVICES_ALLOW_LIST, if set to true will allow other PCI devices to be passed through
#! other than NVIDIA T4 (i.e. "0x10DE:0x1EB8")
VSPHERE_IGNORE_PCI_DEVICES_ALLOW_LIST:
#! VSPHERE_CONTROL_PLANE_CUSTOM_VMX_KEYS, if set will set custom VMX keys on all control plane machines
#! Keys must be set in the form <Key1>=<Value1>,<Key2>=<Value2>
VSPHERE_CONTROL_PLANE_CUSTOM_VMX_KEYS:
#! VSPHERE_WORKER_CUSTOM_VMX_KEYS, if set will set custom VMX keys on all worker machines
#! Keys must be set in the form <Key1>=<Value1>,<Key2>=<Value2>
VSPHERE_WORKER_CUSTOM_VMX_KEYS:
#! WORKER_ROLLOUT_STRATEGY configures the MachineDeployment rollout strategy. If set to OnDelete,
#! on updates, the existing worker machines will be deleted *first* before the replacement worker machines are created.
WORKER_ROLLOUT_STRATEGY: "RollingUpdate"

#! Thumbprint of the vCenter server specified with VSPHERE_SERVER config variable
#! this value can be skipped if user wants to use insecure connection
#! by setting VSPHERE_INSECURE to true
VSPHERE_TLS_THUMBPRINT: ""
VSPHERE_INSECURE: false
#! Virtual IP address or FQDN for the cluster's control plane nodes
VSPHERE_CONTROL_PLANE_ENDPOINT:
VSPHERE_CONTROL_PLANE_ENDPOINT_PORT: 6443
#! NSX-T specific configurations for enabling NSX-T routable pods
#! set this to true to enable NSX-T routable pods
NSXT_POD_ROUTING_ENABLED: false
NSXT_ROUTER_PATH: ""
#! NSX-T username
NSXT_USERNAME: ""
#! NSX-T password
NSXT_PASSWORD: ""
#! NSX-T host
NSXT_MANAGER_HOST: ""
#! set this to true if NSX-T uses self-signed cert
NSXT_ALLOW_UNVERIFIED_SSL: "false"
#! set this to true if NSX-T uses remote authentication (authentication done through the vIDM)
NSXT_REMOTE_AUTH: "false"
NSXT_VMC_ACCESS_TOKEN: ""
NSXT_VMC_AUTH_HOST: ""
NSXT_CLIENT_CERT_KEY_DATA: ""
NSXT_CLIENT_CERT_DATA: ""
NSXT_ROOT_CA_DATA_B64: ""
#! SecretName is the secret name for NSX-T username and password
NSXT_SECRET_NAME: "cloud-provider-vsphere-nsxt-credentials"
#! SecretNamespace is the secret namespace for NSX-T username and password
NSXT_SECRET_NAMESPACE: "kube-system"



#! ---------------------------------------------------------------------
#! Settings for AWS infrastructure provider
#! ---------------------------------------------------------------------
#! For list of supported regions, see TKG documentation.
#! If you have already set an environment variable for the region, you must unset it.
AWS_REGION:
#! AWS_LOAD_BALANCER_SCHEME_INTERNAL will mean that the Kubernetes API Server Load Balancer will not be accessible and routed over the
#! internet.
AWS_LOAD_BALANCER_SCHEME_INTERNAL: false
#! AZ has to be contained in the AWS region
AWS_NODE_AZ: ""
#! set these if you want 3 control plane nodes. The below
#! availability zones must be in the same region as AWS_NODE_AZ
AWS_NODE_AZ_1: ""
AWS_NODE_AZ_2: ""
#! A new VPC will be created for the cluster to be created if this is set blank
#! to use a VPC that already exists in your selected AWS region, enter the ID of the VPC
#! and then set AWS_PUBLIC_SUBNET_ID and AWS_PRIVATE_SUBNET_ID.
#! This option is not required if you set AWS_VPC_CIDR.
AWS_VPC_ID: ""
AWS_PRIVATE_SUBNET_ID: ""
AWS_PUBLIC_SUBNET_ID: ""
AWS_PUBLIC_SUBNET_ID_1: ""
AWS_PRIVATE_SUBNET_ID_1: ""
AWS_PUBLIC_SUBNET_ID_2: ""
AWS_PRIVATE_SUBNET_ID_2: ""
AWS_VPC_CIDR: 10.0.0.0/16
AWS_PRIVATE_NODE_CIDR: 10.0.0.0/24
AWS_PUBLIC_NODE_CIDR: 10.0.1.0/24
AWS_PRIVATE_NODE_CIDR_1: 10.0.2.0/24
AWS_PUBLIC_NODE_CIDR_1: 10.0.3.0/24
AWS_PRIVATE_NODE_CIDR_2: 10.0.4.0/24
AWS_PUBLIC_NODE_CIDR_2: 10.0.5.0/24
#! AWS_SECURITY_GROUP_APISERVER_LB is an optional security group ID of a pre-created
#! security group that will be used for Kubernetes API Server ELB, and will control
#! inbound access to the the control plane endpoint.
AWS_SECURITY_GROUP_APISERVER_LB: ""
#! AWS_SECURITY_GROUP_BASTION is an optional security group ID of a pre-created
#! security group that will be used to control in-bound access to the bastion.
AWS_SECURITY_GROUP_BASTION: ""
#! AWS_SECURITY_GROUP_CONTROLPLANE is an optional security group ID of a pre-created
#! security group that will be used to control in-bound access to the control plane nodes.
#! This group must allow access to port 6443 from AWS_SECURITY_GROUP_APISERVER_LB.
AWS_SECURITY_GROUP_CONTROLPLANE: ""
#! AWS_SECURITY_GROUP_LB is an optional security group for use by the Kubernetes
#! AWS Cloud Provider for setting rules for ELBs.
AWS_SECURITY_GROUP_LB: ""
#! AWS_SECURITY_GROUP_NODE is an optional security group that will be used to
#! to control in-bound acceess to all nodes.
AWS_SECURITY_GROUP_NODE: ""
#! AWS_IDENTITY_REF_KIND is an optional kind of a Kubernetes resource containing
#! an identity to be used for a cluster. Defaults to AWSClusterRoleIdentity if
#! AWS_IDENTITY_REF_NAME is also set.
AWS_IDENTITY_REF_KIND: ""
#! AWS_IDENTITY_REF_NAME is an optional name of a Kubernetes resource containing
#! an identity to be used for a cluster.
AWS_IDENTITY_REF_NAME: ""
#! AWS_CONTROL_PLANE_OS_DISK_SIZE_GIB is the size of the root volume of the
#! control plane instances of a cluster.
AWS_CONTROL_PLANE_OS_DISK_SIZE_GIB: 80
#! AWS_NODE_OS_DISK_SIZE_GIB is the size of the root volume of the
#! node instances of a cluster.
AWS_NODE_OS_DISK_SIZE_GIB: 80

#! Control plane and worker node instance types.
#! For node sizing recommendations, refer to TKG documentation.
CONTROL_PLANE_MACHINE_TYPE: t3.large
NODE_MACHINE_TYPE: m5.large
NODE_MACHINE_TYPE_1: ""
NODE_MACHINE_TYPE_2: ""

#! Name of the SSH private key that you registered with your Amazon EC2 accoun
AWS_SSH_KEY_NAME:

#! Specify "true" to deploy an AWS basion host in the availability zone, or "false" to reuse an existing bastion host
BASTION_HOST_ENABLED: true



#! ---------------------------------------------------------------------
#! Settings for creating clusters on vSphere with Tanzu
#! ---------------------------------------------------------------------
#! Identifies the storage class to be used for storage of the disks that store the root file systems of the worker nodes.
CONTROL_PLANE_STORAGE_CLASS:
#! Specifies the name of the VirtualMachineClass that describes the virtual
#! hardware settings to be used each control plane node in the pool.
CONTROL_PLANE_VM_CLASS:
#! Specifies a named storage class to be annotated as the default in the
#! cluster. If you do not specify it, there is no default.
DEFAULT_STORAGE_CLASS:
#! Specifies the service domain for the cluster
SERVICE_DOMAIN:
#! Specifies named persistent volume (PV) storage classes for container
#! workloads. Storage classes associated with the Supervisor Namespace are
#! replicated in the cluster. In other words, each storage class listed must be
#! available on the Supervisor Namespace for this to be a valid value
STORAGE_CLASSES:
#! Identifies the storage class to be used for storage of the disks that store the root file systems of the worker nodes.
WORKER_STORAGE_CLASS:
#! Specifies the name of the VirtualMachineClass that describes the virtual
#! hardware settings to be used each worker node in the pool
WORKER_VM_CLASS:
#! Specifies the name of the first node pool
NODE_POOL_0_NAME: "workers"
#! Specifies the labels to be applied on first node pool. It should be of the
#! format "key1=value1,key2=value2"
NODE_POOL_0_LABELS:
#! Specifies the taints string to be applied on first node pool. It should be of the
#! format "key1=value1:effect1,key2=value2:effect2"
NODE_POOL_0_TAINTS:



#! ---------------------------------------------------------------------
#! Settings for AZURE infrastructure provider
#! ---------------------------------------------------------------------
#! Azure account configurations

#! The Azure cloud to deploy to, supported clouds are :
#! AzurePublicCloud, AzureChinaCloud, AzureGermanCloud, AzureUSGovernmentCloud
AZURE_ENVIRONMENT: "AzurePublicCloud"
#! The tenant ID is the ID of the AAD directory in which the app for Tanzu Kubernetes Grid is created
#! A Tenant is representative of an organization within Azure Active Directory.
#! It is a dedicated instance of the Azure AD service
AZURE_TENANT_ID:
#! The subscription ID is a GUID that uniquely identifies the subscription to use Azure services.
AZURE_SUBSCRIPTION_ID:
#! The client ID of the app for Tanzu Kubernetes Grid that you registered with Azure
AZURE_CLIENT_ID:
#! The client password of the app for Tanzu Kubernetes Grid that you registered with Azure
AZURE_CLIENT_SECRET:
#! Azure region where the resources will be created
AZURE_LOCATION:
#! SSH to allow for access to the cluster machines, as a base64-encoded string
AZURE_SSH_PUBLIC_KEY_B64:
#! Azure virtual machine size
AZURE_CONTROL_PLANE_MACHINE_TYPE: "Standard_D2s_v3"
AZURE_NODE_MACHINE_TYPE: "Standard_D2s_v3"
#! Accelerated Networking is enabled by default on all supported Azure VMs with 4 or more CPU.
AZURE_ENABLE_ACCELERATED_NETWORKING: true

#! resource-group-name that already exists in your Azure account.
#! AZURE_RESOURCE_GROUP and AZURE_VNET_RESOURCE_GROUP are the same by default.

#! Must be unique to each cluster.
AZURE_RESOURCE_GROUP: ""
#! If unset the value of AZURE_RESOURCE_GROUP will be used as the resource group
#! for the VNET
AZURE_VNET_RESOURCE_GROUP: ""

AZURE_VNET_NAME: ""
AZURE_VNET_CIDR: "10.0.0.0/16"
AZURE_CONTROL_PLANE_SUBNET_NAME: ""
AZURE_CONTROL_PLANE_SUBNET_CIDR: "10.0.0.0/24"
AZURE_CONTROL_PLANE_SUBNET_SECURITY_GROUP: ""
AZURE_NODE_SUBNET_NAME: ""
AZURE_NODE_SUBNET_CIDR: "10.0.1.0/24"
AZURE_NODE_SUBNET_SECURITY_GROUP: ""

#! AZ has to be contained in the Azure location.
#! If not provided, the default value for AZURE_NODE_AZ is 1, AZURE_NODE_AZ_1 is 2 and AZURE_NODE_AZ_2 is 3
AZURE_NODE_AZ: ""
#! set these if you want to customize the AZs for a prod cluster. The below
#! availability zones must be in the same region as AZURE_NODE_AZ
AZURE_NODE_AZ_1: ""
AZURE_NODE_AZ_2: ""

#! set of tags to add to Azure resources managed by the Azure provider
AZURE_CUSTOM_TAGS:

#! disk configuration
AZURE_CONTROL_PLANE_OS_DISK_SIZE_GIB: 128
AZURE_CONTROL_PLANE_OS_DISK_STORAGE_ACCOUNT_TYPE: Premium_LRS
AZURE_NODE_OS_DISK_SIZE_GIB: 128
AZURE_NODE_OS_DISK_STORAGE_ACCOUNT_TYPE: Premium_LRS

AZURE_CONTROL_PLANE_DATA_DISK_SIZE_GIB: 256
AZURE_ENABLE_NODE_DATA_DISK: false
AZURE_NODE_DATA_DISK_SIZE_GIB: 256

#! Deploy private Azure clusters.
#! A Private cluster will have an Azure internal load balancer load balancing traffic inside the VNet to the control plane nodes.
AZURE_ENABLE_PRIVATE_CLUSTER: false
AZURE_FRONTEND_PRIVATE_IP: "10.0.0.100"
AZURE_ENABLE_CONTROL_PLANE_OUTBOUND_LB: false
AZURE_ENABLE_NODE_OUTBOUND_LB: false
AZURE_CONTROL_PLANE_OUTBOUND_LB_FRONTEND_IP_COUNT: 1
AZURE_NODE_OUTBOUND_LB_FRONTEND_IP_COUNT: 1
#! specifies the number of minutes to keep a TCP connection open for the outbound rule
#! https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-tcp-reset#configurable-tcp-idle-timeout
AZURE_NODE_OUTBOUND_LB_IDLE_TIMEOUT_IN_MINUTES: 4

#! Azure VM Image Config values
#! Azure image ID
AZURE_IMAGE_ID:
#! Azure shared image gallery
AZURE_IMAGE_RESOURCE_GROUP:
AZURE_IMAGE_NAME:
AZURE_IMAGE_SUBSCRIPTION_ID:
AZURE_IMAGE_GALLERY:
#! Azure marketplace image
AZURE_IMAGE_PUBLISHER:
AZURE_IMAGE_OFFER:
AZURE_IMAGE_SKU:
AZURE_IMAGE_THIRD_PARTY:
#! For both marketplace and shared image gallery
AZURE_IMAGE_VERSION:

#! Azure multi-tenancy
AZURE_IDENTITY_NAME:
AZURE_IDENTITY_NAMESPACE:

#! ---------------------------------------------------------------------
#! OIDC-related configuration
#! Set these values to enable OIDC on management cluster.
#! ---------------------------------------------------------------------
#! ENABLE_OIDC is either true or false. It can be set by user or by tkg cli option --enable-cluster-options="oidc"
ENABLE_OIDC: false
#! OIDC_ISSUER_URL is https://<KUBERNETES_API_SERVER_ENDPOINT>:30167
#! OIDC_USERNAME_CLAIM is JWT claim to use as the user name. e.g, sub, email or name.
#! OIDC_GROUPS_CLAIM is JWT claim to use as the user's group.
#! OIDC_DEX_CA can be obtained by retrieving the secret dex-cert-tls from namespace tanzu-system-auth in the management cluster
#! e.g. kubectl get secret dex-cert-tls -n tanzu-system-auth -o 'go-template={{ index .data "ca.crt" }}' | base64 -D | gzip | base64
OIDC_ISSUER_URL:
OIDC_USERNAME_CLAIM:
OIDC_GROUPS_CLAIM:
OIDC_DEX_CA:



#! ---------------------------------------------------------------------
#! Machine Health Check configuration
#! ---------------------------------------------------------------------
ENABLE_MHC:
ENABLE_MHC_WORKER_NODE: true
ENABLE_MHC_CONTROL_PLANE: true
MHC_UNKNOWN_STATUS_TIMEOUT: 5m
MHC_FALSE_STATUS_TIMEOUT: 12m

#! Common configuration
#! ---------------------------------------------------------------------

#! Set this value to configure TKG to use a custom image repository for any
#! container image that needs to be pulled during cluster creation. This is
#! useful when deploying in an internet-restricted environment with the help
#! of a private container registry.
#! See TKG documentation for more information about preparing a private
#! container repository.
TKG_CUSTOM_IMAGE_REPOSITORY: ""
TKG_CUSTOM_IMAGE_REPOSITORY_SKIP_TLS_VERIFY: false
TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE: ""

#! PROXY configuration
#! If `TKG_PROXY_CA_CERT` is specified it gets used instead of
#! `TKG_CUSTOM_IMAGE_REPOSITORY_CA_CERTIFICATE` while communicating
#! with the registry
TKG_HTTP_PROXY: ""
TKG_HTTPS_PROXY: ""
TKG_NO_PROXY: ""
TKG_PROXY_CA_CERT: ""

#! IP Family setting
TKG_IP_FAMILY:

#! Configure cloud provider permissions for TMC enablement. Only affects AWS at present.
DISABLE_TMC_CLOUD_PERMISSIONS: false

#! Node Nameservers (currently only supports the vSphere provider)
CONTROL_PLANE_NODE_NAMESERVERS:
WORKER_NODE_NAMESERVERS:

#! Audit Logging settings
ENABLE_AUDIT_LOGGING: false

#! Configures default storage class for workload cluster if enabled
ENABLE_DEFAULT_STORAGE_CLASS: true

#! CIDR default values are set based on TKG_IP_FAMILY in the CLI
CLUSTER_CIDR:
SERVICE_CIDR:
NODE_STARTUP_TIMEOUT: 20m

CONTROL_PLANE_MACHINE_COUNT:
WORKER_MACHINE_COUNT:
#! sets the number of worker nodes in the first AZ, AWS_NODE_AZ.
WORKER_MACHINE_COUNT_0:
#! sets the number of worker nodes in the second AZ, AWS_NODE_AZ_1.
WORKER_MACHINE_COUNT_1:
#! sets the number of worker nodes in the third AZ, AWS_NODE_AZ_2.
WORKER_MACHINE_COUNT_2:

#! OS selection configuration
OS_NAME: ""
OS_VERSION: ""
OS_ARCH: ""



#! ---------------------------------------------------------------------
#! Autoscaler related configuration
#! ---------------------------------------------------------------------
ENABLE_AUTOSCALER: false
AUTOSCALER_MAX_NODES_TOTAL: "0"
AUTOSCALER_SCALE_DOWN_DELAY_AFTER_ADD: "10m"
AUTOSCALER_SCALE_DOWN_DELAY_AFTER_DELETE: "10s"
AUTOSCALER_SCALE_DOWN_DELAY_AFTER_FAILURE: "3m"
AUTOSCALER_SCALE_DOWN_UNNEEDED_TIME: "10m"
AUTOSCALER_MAX_NODE_PROVISION_TIME: "15m"
AUTOSCALER_MIN_SIZE_0:
AUTOSCALER_MAX_SIZE_0:
AUTOSCALER_MIN_SIZE_1:
AUTOSCALER_MAX_SIZE_1:
AUTOSCALER_MIN_SIZE_2:
AUTOSCALER_MAX_SIZE_2:



#! ---------------------------------------------------------------------
#! Settings for Docker infrastructure provider
#! ---------------------------------------------------------------------
DOCKER_MACHINE_TEMPLATE_IMAGE:



#! ---------------------------------------------------------------------
#! Pinniped identity management configuration
#! ---------------------------------------------------------------------
#! Identifies the Identity management type to be deployed. Possible values are "oidc", "ldap" or "none"
#! If "none" is specified, pinniped services will not be deployed on the cluster
IDENTITY_MANAGEMENT_TYPE: "none"

#! ---------------------------------------------------------------------
#! SecretGen Controller configuration
#! ---------------------------------------------------------------------
SECRETGEN_CONTROLLER_ENABLE: true

#! Settings for Pinniped OIDC
CERT_DURATION: 2160h
CERT_RENEW_BEFORE: 360h
OIDC_IDENTITY_PROVIDER_NAME:
OIDC_IDENTITY_PROVIDER_ISSUER_URL:
OIDC_IDENTITY_PROVIDER_CLIENT_ID:
OIDC_IDENTITY_PROVIDER_CLIENT_SECRET:
#! comma separated list of scopes apart from openid e.g:"email,offline_access"
OIDC_IDENTITY_PROVIDER_SCOPES: "email,profile,groups"
OIDC_IDENTITY_PROVIDER_USERNAME_CLAIM:
OIDC_IDENTITY_PROVIDER_GROUPS_CLAIM:
#! The following two variables are used to configure Pinniped JWTAuthenticator for workload clusters
SUPERVISOR_ISSUER_URL:
SUPERVISOR_ISSUER_CA_BUNDLE_DATA_B64:

#! Settings for LDAP (Pinniped configured with Dex)
LDAP_BIND_DN:
LDAP_BIND_PASSWORD:
LDAP_HOST:
LDAP_USER_SEARCH_BASE_DN:
LDAP_USER_SEARCH_FILTER:
LDAP_USER_SEARCH_USERNAME: userPrincipalName
LDAP_USER_SEARCH_ID_ATTRIBUTE: DN
LDAP_USER_SEARCH_EMAIL_ATTRIBUTE: DN
LDAP_USER_SEARCH_NAME_ATTRIBUTE:
LDAP_GROUP_SEARCH_BASE_DN:
LDAP_GROUP_SEARCH_FILTER:
LDAP_GROUP_SEARCH_USER_ATTRIBUTE: DN
LDAP_GROUP_SEARCH_GROUP_ATTRIBUTE:
LDAP_GROUP_SEARCH_NAME_ATTRIBUTE: cn
LDAP_ROOT_CA_DATA_B64:



#! ---------------------------------------------------------------------
#! AVI NSX Advanced Loader Balancer configuration
#! ---------------------------------------------------------------------
#! Whether to use Avi aka. NSX Advanced Loader Balancer or not
AVI_ENABLE: false

#! Whether to use Avi aka. NSX Advanced Loader Balancer to provide clusters'
#! control plane nodes load balancing or not
AVI_CONTROL_PLANE_HA_PROVIDER: false

#! Settings for Avi aka. NSX Advanced Loader Balancer controller

#! Avi aka. NSX Advanced Loader Balancer's controller ip or FQDN
AVI_CONTROLLER:
AVI_CONTROLLER_VERSION: 20.1.3
AVI_USERNAME: ""
AVI_PASSWORD: ""
#! The cerficiate used to access Avi aka. NSX Advanced Loader Balancer's controller, need to be base64 encoded
AVI_CA_DATA_B64: ""
#! Which infrastructure cloud will be used for TKG, currently support two type clouds: vcenter|NSX-T
AVI_CLOUD_NAME:
#! t1 router specifies the path for NSX Advanced Loader Balancer backend network, only applies when configure NSX ALB to NSX-T cloud
AVI_NSXT_T1LR:


#!  Settings for workload clusters' load balancer VIP networks:
AVI_DATA_NETWORK:
AVI_DATA_NETWORK_CIDR:
AVI_CONTROL_PLANE_NETWORK:
AVI_CONTROL_PLANE_NETWORK_CIDR:
AVI_SERVICE_ENGINE_GROUP:


#! Settings for management cluster's load balancer VIP networks:
#! All fields in this section are optional, if leave them empty,
#! management cluster will use the same load balancer VIP network
#! configurations as workload clusters
AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_NAME:
AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_CIDR:
AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_NAME:
AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_CIDR:
AVI_MANAGEMENT_CLUSTER_SERVICE_ENGINE_GROUP:


#! Settings for ako-operator components
AVI_NAMESPACE: "tkg-system-networking"
AVI_DISABLE_INGRESS_CLASS: true
AVI_AKO_IMAGE_PULL_POLICY: IfNotPresent
AVI_ADMIN_CREDENTIAL_NAME: avi-controller-credentials
AVI_CA_NAME: avi-controller-ca
#! AVI_LABELS specify default install-ako-for-all cluster selector matching labels
AVI_LABELS:


#!  Settings for NSX Advanced Loader Balancer L7 load balancing configurations, which requires NSX Advanced Loader Balancer Enterprise license tier
AVI_DISABLE_STATIC_ROUTE_SYNC: true
AVI_INGRESS_DEFAULT_INGRESS_CONTROLLER: false
AVI_INGRESS_SHARD_VS_SIZE: ""
AVI_INGRESS_SERVICE_TYPE: ""
AVI_INGRESS_NODE_NETWORK_LIST: ""


#! ---------------------------------------------------------------------
#! Antrea CNI configuration
#! ---------------------------------------------------------------------
ANTREA_NO_SNAT: false
ANTREA_DISABLE_UDP_TUNNEL_OFFLOAD: false
ANTREA_TRAFFIC_ENCAP_MODE: "encap"
ANTREA_EGRESS_EXCEPT_CIDRS: ""
ANTREA_NODEPORTLOCAL_ENABLED: true
ANTREA_NODEPORTLOCAL_PORTRANGE: 61000-62000
ANTREA_PROXY_ALL: false
ANTREA_PROXY_NODEPORT_ADDRS: ""
ANTREA_PROXY_SKIP_SERVICES: ""
ANTREA_PROXY_LOAD_BALANCER_IPS: false
ANTREA_FLOWEXPORTER_COLLECTOR_ADDRESS: "flow-aggregator.flow-aggregator.svc:4739:tls"
ANTREA_FLOWEXPORTER_POLL_INTERVAL: "5s"
ANTREA_FLOWEXPORTER_ACTIVE_TIMEOUT: "30s"
ANTREA_FLOWEXPORTER_IDLE_TIMEOUT: "15s"
ANTREA_KUBE_APISERVER_OVERRIDE:
ANTREA_TRANSPORT_INTERFACE:
ANTREA_TRANSPORT_INTERFACE_CIDRS: ""
ANTREA_MULTICAST_INTERFACES: ""
ANTREA_TUNNEL_TYPE: geneve
ANTREA_TRAFFIC_ENCRYPTION_MODE: none
ANTREA_WIREGUARD_PORT: 51820
ANTREA_ENABLE_USAGE_REPORTING: false

#! Antrea feature gates
ANTREA_PROXY: true
ANTREA_ENDPOINTSLICE: true
ANTREA_POLICY: true
ANTREA_TRACEFLOW: true
ANTREA_NODEPORTLOCAL: true
ANTREA_NETWORKPOLICY_STATS: false
ANTREA_EGRESS: true
ANTREA_IPAM: false
ANTREA_FLOWEXPORTER: false
ANTREA_SERVICE_EXTERNALIP: false
ANTREA_MULTICAST: false


#! ---------------------------------------------------------------------
#! Internal configuration, not meant to be overridden
#! ---------------------------------------------------------------------
TKG_DEFAULT_BOM:
PROVIDER_TYPE:
TKG_CLUSTER_ROLE:
TKG_VERSION:
FILTER_BY_ADDON_TYPE:
REMOVE_CRS_FOR_ADDON_TYPE:
DISABLE_CRS_FOR_ADDON_TYPE:
CNI: antrea
KUBERNETES_RELEASE:
KUBERNETES_VERSION: v1.19.1+vmware.2
#! This is autofilled by TKG CLI
AWS_AMI_ID:
TKR_DISCOVER_FREQUENCY: 600
#! vSphere version
VSPHERE_VERSION:
#! CORE_DNS_IP is the IP of core DNS service, supports both IPv4 and IPv6
#! It is the 10th index of SERVICE_CIDR subnet
CORE_DNS_IP:
#! This defines feature flag package-based-lcm is enabled or not
FEATURE_FLAG_PACKAGE_BASED_LCM:
#! Used for testing clusterclasses only
TKR_DATA:

#! ---------------------------------------------------------------------
#! BoM file processing, internal use only
#! ---------------------------------------------------------------------

#@ load("@ytt:yaml", "yaml")
#@ load("@ytt:data", "data")

#@ files = data.list()
boms:
    #@ for/end file in [ f for f in files if f.startswith("tkg-bom") or  f.startswith("tkr-bom") or f.startswith("bom")]:
    - bom_name: #@ file
      bom_data: #@ yaml.decode(data.read(file))
